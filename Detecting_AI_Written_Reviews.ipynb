{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCLoy083pBzg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/mal_training_data_hum_ai.csv')\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Step 2: Preprocess the text data\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define a preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Remove unwanted characters (punctuations, special characters)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Convert to lowercase (if needed for Tamil)\n",
        "    text = text.lower()\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the DATA column\n",
        "df['DATA'] = df['DATA'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X = df['DATA']  # Text data\n",
        "y = df['LABEL']  # Labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Convert text into numerical format using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), analyzer='char')  # Character-level n-grams for Tamil\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 5: Build and train a classification model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "fWDnCrD3qJOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "QPyd7LejqL6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the dataset\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if \"DATA\" not in df.columns or \"LABEL\" not in df.columns:\n",
        "            raise ValueError(\"Dataset must contain 'DATA' and 'LABEL' columns.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Remove unwanted characters (punctuations, special characters)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Convert to lowercase (if needed for Tamil)\n",
        "    text = text.lower()\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Load and preprocess dataset\n",
        "file_path = '/content/drive/MyDrive/tam_training_data_hum_ai.csv'\n",
        "df = load_dataset(file_path)\n",
        "if df is not None:\n",
        "    df['DATA'] = df['DATA'].fillna('').apply(preprocess_text)  # Fill NaN with empty strings\n",
        "\n",
        "    # Map labels to integers\n",
        "    label_mapping = {'AI': 0, 'HUMAN': 1}\n",
        "    if not set(df['LABEL'].unique()).issubset(label_mapping.keys()):\n",
        "        raise ValueError(\"Invalid labels in the dataset. Expected labels are: 'AI' and 'HUMAN'.\")\n",
        "    df['LABEL'] = df['LABEL'].map(label_mapping)\n",
        "\n",
        "    # Convert to Hugging Face Dataset\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    # Cast LABEL column to ClassLabel\n",
        "    class_label = ClassLabel(num_classes=2, names=[\"AI\", \"HUMAN\"])\n",
        "    dataset = dataset.cast_column(\"LABEL\", class_label)\n",
        "\n",
        "    # Perform train-test split\n",
        "    train_test = dataset.train_test_split(test_size=0.2, stratify_by_column=\"LABEL\")\n",
        "    train_test = DatasetDict({\"train\": train_test[\"train\"], \"test\": train_test[\"test\"]})\n",
        "\n",
        "    # Step 3: Load the tokenizer and model\n",
        "    model_name = \"bert-base-multilingual-cased\"  # Change to Indic-specific models if required\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Tokenization function\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"DATA\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "    tokenized_datasets = train_test.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns([\"DATA\"])  # Only remove the 'DATA' column\n",
        "    tokenized_datasets = tokenized_datasets.rename_column(\"LABEL\", \"labels\")\n",
        "    tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "    # Step 4: Define metrics\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = pred.predictions.argmax(-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        f1 = f1_score(labels, preds)\n",
        "        precision = precision_score(labels, preds)\n",
        "        recall = recall_score(labels, preds)\n",
        "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "        # Step 5: Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "        save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=1,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "        metric_for_best_model=\"f1\",\n",
        "        report_to=\"none\"  # Disable integration with platforms like Weights & Biases or TensorBoard if not needed\n",
        "    )\n",
        "\n",
        "    # Step 6: Train the model\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Step 7: Evaluate on test set\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation Results:\", eval_results)\n",
        "else:\n",
        "    print(\"Failed to load the dataset. Please check the file path and format.\")"
      ],
      "metadata": {
        "id": "uuDT5zhxqOR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./saved_model\")\n",
        "tokenizer.save_pretrained(\"./saved_model\")"
      ],
      "metadata": {
        "id": "eXT47yOoqcDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the test dataset\n",
        "def load_test_dataset(file_path):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        if \"ID\" not in df.columns or \"DATA\" not in df.columns:\n",
        "            raise ValueError(\"Test dataset must contain 'ID' and 'DATA' columns.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading test dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    import re\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = text.lower()                  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "def predict_labels(test_file_path, model_path, tokenizer_path):\n",
        "    test_df = load_test_dataset(test_file_path)\n",
        "    if test_df is None:\n",
        "        return\n",
        "\n",
        "    test_df['DATA'] = test_df['DATA'].fillna('').apply(preprocess_text)  # Handle missing data\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        test_df['DATA'].tolist(),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    label_mapping = {0: \"AI\", 1: \"HUMAN\"}  # Adjust based on your training setup\n",
        "    test_df['PREDICTED_LABEL'] = predictions.numpy()\n",
        "    test_df['PREDICTED_LABEL'] = test_df['PREDICTED_LABEL'].map(label_mapping)\n",
        "\n",
        "    output_file = \"predicted_test_results.csv\"\n",
        "    test_df.to_csv(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")"
      ],
      "metadata": {
        "id": "EaO1xFQDqcRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_path = '/content/drive/MyDrive/tam_test_data_hum_ai.xlsx'\n",
        "model_path = '/content/saved_model'\n",
        "tokenizer_path = '/content/saved_model'\n",
        "predict_labels(test_file_path, model_path, tokenizer_path)"
      ],
      "metadata": {
        "id": "9qIeNfpKqkrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "test_df = pd.read_excel('/content/drive/MyDrive/mal_test_data_hum_ai.xlsx')\n",
        "\n",
        "# Ensure the test dataset contains 'ID' and 'DATA' columns\n",
        "if 'ID' not in test_df.columns or 'DATA' not in test_df.columns:\n",
        "    raise ValueError(\"Test dataset must contain 'ID' and 'DATA' columns.\")"
      ],
      "metadata": {
        "id": "zrImjt03q1US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    # Remove unwanted characters (punctuations, special characters)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Convert to lowercase (useful for case-insensitive languages)\n",
        "    text = text.lower()\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the 'DATA' column\n",
        "test_df['DATA'] = test_df['DATA'].fillna('').apply(preprocess_text)  # Fill missing values with empty strings\n",
        "\n",
        "# Step 3: Transform the test data using the trained TF-IDF vectorizer\n",
        "X_test = test_df['DATA']\n",
        "\n",
        "try:\n",
        "    X_test_tfidf = vectorizer.transform(X_test)  # Use the TF-IDF vectorizer trained on training data\n",
        "except NameError:\n",
        "    raise ValueError(\"TF-IDF vectorizer is not defined. Ensure it's loaded from the training script.\")\n",
        "\n",
        "# Step 4: Predict labels for the test dataset\n",
        "try:\n",
        "    y_pred = model.predict(X_test_tfidf)  # Use the trained model\n",
        "except NameError:\n",
        "    raise ValueError(\"Model is not defined. Ensure it's loaded from the training script.\")\n",
        "\n",
        "# Step 5: Add predictions to the test dataset\n",
        "test_df['PREDICTED_LABEL'] = y_pred\n",
        "\n",
        "# Map predicted labels back to their original names if necessary (optional)\n",
        "# Uncomment and modify if you used encoded labels during training\n",
        "# label_mapping = {0: 'AI', 1: 'HUMAN'}\n",
        "# test_df['PREDICTED_LABEL'] = test_df['PREDICTED_LABEL'].map(label_mapping)\n",
        "\n",
        "# Step 6: Save predictions to a CSV file\n",
        "output_file = '/content/drive/MyDrive/test_predictions_1.csv'  # Specify the output path\n",
        "test_df.to_csv(output_file, index=False)\n",
        "print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "# Step 7: Display a few predictions (optional)\n",
        "print(test_df[['ID', 'DATA', 'PREDICTED_LABEL']].head())\n",
        "\n",
        "print(df['LABEL'].value_counts())"
      ],
      "metadata": {
        "id": "Y5bXHH_Wq5Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torch\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "def load_and_preprocess_dataset(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if \"DATA\" not in df.columns or \"LABEL\" not in df.columns:\n",
        "            raise ValueError(\"Dataset must contain 'DATA' and 'LABEL' columns.\")\n",
        "\n",
        "        # Preprocess text data\n",
        "        def preprocess_text(text):\n",
        "            text = re.sub(r'[^஀-௿\\w\\s]', '', text)  # Remove non-Tamil characters\n",
        "            text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "            text = text.lower()  # Convert to lowercase\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "            return text\n",
        "\n",
        "        df['DATA'] = df['DATA'].fillna('').apply(preprocess_text)\n",
        "\n",
        "        # Map labels to integers\n",
        "        label_mapping = {'AI': 0, 'HUMAN': 1}\n",
        "        df['LABEL'] = df['LABEL'].map(label_mapping)\n",
        "\n",
        "        # Convert to Hugging Face Dataset\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "        class_label = ClassLabel(num_classes=2, names=[\"AI\", \"HUMAN\"])\n",
        "        dataset = dataset.cast_column(\"LABEL\", class_label)\n",
        "\n",
        "        # Split into training and testing sets\n",
        "        train_test = dataset.train_test_split(test_size=0.2, stratify_by_column=\"LABEL\")\n",
        "        return DatasetDict({\"train\": train_test[\"train\"], \"test\": train_test[\"test\"]})\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 2: Tokenization and Model Setup\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"DATA\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "# Step 3: Define Metrics for Evaluation\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
        "        \"recall\": recall_score(labels, preds, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "# Step 4: Load Dataset and Tokenizer\n",
        "file_path = '/content/drive/MyDrive/tam_training_data_hum_ai.csv'\n",
        "dataset = load_and_preprocess_dataset(file_path)\n",
        "if dataset is None:\n",
        "    raise ValueError(\"Failed to load the dataset.\")\n",
        "\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"DATA\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"LABEL\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# Step 5: Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Step 6: Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Step 7: Train the Model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "6m0c9SPSrEfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Model and Tokenizer\n",
        "model.save_pretrained(\"./saved_model\")\n",
        "tokenizer.save_pretrained(\"./saved_model\")\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n",
        "\n",
        "# Step 9: Predict on New Data\n",
        "def predict_labels(test_file_path, model_path, tokenizer_path):\n",
        "    test_df = pd.read_excel(test_file_path)\n",
        "    if \"ID\" not in test_df.columns or \"DATA\" not in test_df.columns:\n",
        "        raise ValueError(\"Test dataset must contain 'ID' and 'DATA' columns.\")\n",
        "\n",
        "    test_df['DATA'] = test_df['DATA'].fillna('').apply(preprocess_text)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        test_df['DATA'].tolist(),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    label_mapping = {0: \"AI\", 1: \"HUMAN\"}\n",
        "    test_df['PREDICTED_LABEL'] = predictions.numpy()\n",
        "    test_df['PREDICTED_LABEL'] = test_df['PREDICTED_LABEL'].map(label_mapping)\n",
        "\n",
        "    output_file = \"predicted_test_results.csv\"\n",
        "    test_df.to_csv(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")"
      ],
      "metadata": {
        "id": "TQhG30-QrQCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_path = '/content/drive/MyDrive/tam_test_data_hum_ai.xlsx'\n",
        "model_path = './saved_model'\n",
        "tokenizer_path = './saved_model'\n",
        "predict_labels(test_file_path, model_path, tokenizer_path)"
      ],
      "metadata": {
        "id": "fTrz4um-rSuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    precision = precision_score(labels, preds, average=\"weighted\")\n",
        "    recall = recall_score(labels, preds, average=\"weighted\")\n",
        "\n",
        "    # Print the classification report\n",
        "    report = classification_report(labels, preds, target_names=[\"AI\", \"HUMAN\"])\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    }"
      ],
      "metadata": {
        "id": "WfN65a-mrWJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Step 8: Manually perform predictions and compute metrics\n",
        "trainer.model.eval()\n",
        "eval_preds, eval_labels = [], []\n",
        "\n",
        "# Get the evaluation DataLoader from the Trainer\n",
        "eval_dataloader = trainer.get_eval_dataloader()\n",
        "\n",
        "# Iterate over the DataLoader\n",
        "for batch in eval_dataloader:\n",
        "    # Extract the input and labels from the batch\n",
        "    inputs = {key: value.to(trainer.args.device) for key, value in batch.items() if key != 'labels'}\n",
        "    labels = batch['labels'].to(trainer.args.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        outputs = trainer.model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    eval_preds.extend(preds.cpu().numpy())\n",
        "    eval_labels.extend(labels.cpu().numpy())"
      ],
      "metadata": {
        "id": "6tSqWXbLrZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the classification report\n",
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(eval_labels, eval_preds, target_names=[\"AI\", \"HUMAN\"])\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "7xvKDXflrbgX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}